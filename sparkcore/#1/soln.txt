EXERCISE 1: Load the data from file “userdata.txt” into HDFS.
-------------------------------------------------------------
import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
//Creating RDD from input data
val data = sc.textFile("location of userdata")
.map(line => line.split(","))
.map(userRecord => (userRecord(0), userRecord(1), userRecord(2),userRecord(3),userRecord(4)))


---------------------------------------------------------------
EXERCISE 2: Return the RDD as a set of tuples
---------------------------------------------------------------
data.collect().foreach(println)

---------------------------------------------------------------
EXERCISE 3: Find the number of unique professions in the data file
---------------------------------------------------------------
val uniqueProfessions = data.map {case (id, age, gender, profession,zipcode) => profession}.distinct().count()
println(s"Number of unique professionals $uniqueProfessions \n")

----------------------------------------------------------------
EXERCISE 4: How many different users belong to unique profession
----------------------------------------------------------------
val usersByProfession = data.map{ case (id, age, gender, profession,zipcode) => (profession, 1) }.reduceByKey(_ + _).sortBy(-_._2)
println("Users group by professions")
usersByProfession.collect().foreach(println)

----------------------------------------------------------------
EXERCISE 5: How many different users belong to unique zip code
----------------------------------------------------------------
val usersByZipCode = data.map{ case (id, age, gender, profession,zipcode) => (zipcode, 1) }.reduceByKey(_ + _).sortBy(-_._2)
println(s"Users group by Zip Codes")
usersByZipCode.collect().foreach(println)